{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The implementation of a Tic-Tac-Toe game where a human competes against an AI utilizing Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions on playing this game\n",
    "\n",
    "#### Start the Game: Make Your Move by inputing row followed by column (0-2), divided by a space corresponding to the grid cells, from top-left to bottom-right, to place your marker (1).\n",
    "#### Understand the AI: You're playing against an advanced AI (-1) that learns and predicts optimal moves using Q-Learning technique.\n",
    "#### Play to Win: Aim to align three of your markers (1) in a row, column, or diagonal to win.\n",
    "#### Game End: The game ends when one player wins by aligning three markers or when all grid spaces are filled, resulting in a draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Active status:\n",
      " [[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "\n",
      " Input row followed by column (0-2), divided by a space: 1 0\n",
      "\n",
      " Active status:\n",
      " [[0 0 0]\n",
      " [1 0 0]\n",
      " [0 0 0]]\n",
      "\n",
      " AI is making a move\n",
      "\n",
      " Active status:\n",
      " [[-1  0  0]\n",
      " [ 1  0  0]\n",
      " [ 0  0  0]]\n",
      "\n",
      " Input row followed by column (0-2), divided by a space: 1 1\n",
      "\n",
      " Active status:\n",
      " [[-1  0  0]\n",
      " [ 1  1  0]\n",
      " [ 0  0  0]]\n",
      "\n",
      " AI is making a move\n",
      "\n",
      " Active status:\n",
      " [[-1 -1  0]\n",
      " [ 1  1  0]\n",
      " [ 0  0  0]]\n",
      "\n",
      " Input row followed by column (0-2), divided by a space: 1 2\n",
      "You win!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialize the Q-values dictionary to store state-action pairs and their values\n",
    "Q = {}\n",
    "\n",
    "# Learning parameters\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "exploration_rate = 0.1\n",
    "\n",
    "# Initialize the game state as a 3x3 matrix of zeros\n",
    "def initialize_game_state():\n",
    "    return np.zeros((3, 3), dtype=int)\n",
    "\n",
    "# Define possible actions based on the current state (empty cells)\n",
    "def available_actions(state):\n",
    "    return [(i, j) for i in range(3) for j in range(3) if state[i][j] == 0]\n",
    "\n",
    "# Function to calculate the reward for the current state\n",
    "def calculate_reward(state, player):\n",
    "    # Check for a win for the current player\n",
    "    if (np.any(np.all(state == player, axis=0)) or\n",
    "        np.any(np.all(state == player, axis=1)) or\n",
    "        np.all(np.diag(state) == player) or\n",
    "        np.all(np.diag(np.fliplr(state)) == player)):\n",
    "        return 1  # Reward for winning\n",
    "    if np.all(state != 0):\n",
    "        return 0.5  # Reward for a tie\n",
    "    return 0  # No reward if the game is still ongoing\n",
    "\n",
    "# Epsilon-greedy algorithm to choose the next action\n",
    "def select_action(state, exploration_rate):\n",
    "    if random.random() < exploration_rate:\n",
    "        return random.choice(available_actions(state))\n",
    "    else:\n",
    "        # Return the best action based on Q-values\n",
    "        return max(available_actions(state), key=lambda action: Q.get((tuple(map(tuple, state)), action), 0))\n",
    "\n",
    "# Q-learning update formula\n",
    "def update_Q_values(prev_state, action, reward, next_state):\n",
    "    prev_state = tuple(map(tuple, prev_state))\n",
    "    next_state = tuple(map(tuple, next_state))\n",
    "    best_next_action = max(available_actions(next_state), key=lambda x: Q.get((next_state, x), 0), default=(0, 0))\n",
    "    td_target = reward + discount_factor * Q.get((next_state, best_next_action), 0)\n",
    "    td_error = td_target - Q.get((prev_state, action), 0)\n",
    "    Q[(prev_state, action)] = Q.get((prev_state, action), 0) + learning_rate * td_error\n",
    "\n",
    "# Main function to execute the game loop\n",
    "def play_game():\n",
    "    state = initialize_game_state()\n",
    "    game_over = False\n",
    "    current_player = 1  # Start with player 1\n",
    "\n",
    "    while not game_over:\n",
    "        print(\"\\n Active status:\\n\", state)\n",
    "        if current_player == 1:\n",
    "            try:\n",
    "                row, col = map(int, input(\"\\n Input row followed by column (0-2), divided by a space: \").split())\n",
    "                assert state[row][col] == 0, \"This cell is already taken. Please try a different move.\"\n",
    "                state[row][col] = current_player  # Player's move\n",
    "            except (ValueError, AssertionError) as e:\n",
    "                print(str(e), \"Please try again.\")\n",
    "                continue\n",
    "        else:\n",
    "            print(\"\\n AI is making a move\")\n",
    "            action = select_action(state, exploration_rate)\n",
    "            state[action[0]][action[1]] = current_player  # AI's move\n",
    "\n",
    "        reward = calculate_reward(state, current_player)\n",
    "        if reward > 0:\n",
    "            print(\"You\" if current_player == 1 else \"AI\", \"win!\")\n",
    "            game_over = True\n",
    "        elif np.all(state != 0):\n",
    "            print(\"It's a draw!\")\n",
    "            game_over = True\n",
    "        else:\n",
    "            current_player = -1 if current_player == 1 else 1  # Switch player\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    play_game()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking down the code step-by-step:\n",
    "\n",
    "Imports and Initial Setup:\n",
    "numpy: Used for matrix operations, which are ideal for representing the game board.\n",
    "random: Used for generating random numbers, which are essential for implementing exploration in the Q-learning algorithm.\n",
    "\n",
    "Q-dictionary Initialization:\n",
    "Q: A dictionary that stores the state-action pairs as keys and their corresponding Q-values as values.\n",
    "Learning Parameters:\n",
    "\n",
    "learning_rate (α): The rate at which the AI learns. A higher rate means the AI updates its Q-values more aggressively based on new information.\n",
    "\n",
    "discount_factor (γ): Represents the importance of future rewards. A factor close to 1 means future rewards are almost as important as immediate rewards.\n",
    "\n",
    "exploration_rate (ε): The probability of choosing a random action. This parameter balances exploration (trying new actions) with exploitation (choosing known best actions).\n",
    "\n",
    "initialize_game_state(): Initializes the game board as a 3x3 matrix filled with zeros. In Tic-Tac-Toe, '0' can represent an empty cell, while other numbers (e.g., 1 and -1) represent different players.\n",
    "\n",
    "available_actions(state): Returns a list of all possible actions (empty cells) available in the current state of the board.\n",
    "\n",
    "calculate_reward(state, player): Calculates the reward for the given state and player. The player gets a reward of 1 for winning, 0.5 for a draw (when the board is full and no one has won), and 0 for ongoing games.\n",
    "\n",
    "select_action(state, exploration_rate): Implements the ε-greedy strategy for action selection:\n",
    "With probability ε, it chooses a random action (exploration),\n",
    "With probability 1-ε, it chooses the best action based on the Q-values (exploitation).\n",
    "\n",
    "update_Q_values(prev_state, action, reward, next_state): Updates the Q-values using the Q-learning formula:\n",
    "TD Target: The sum of the reward and the discounted maximum Q-value of the next state,\n",
    "TD Error: The difference between the TD target and the current Q-value of the action taken,\n",
    "Updates the Q-value by adjusting it in the direction of the TD error, scaled by the learning rate.\n",
    "\n",
    "play_game(): The main game loop where the game is played until it ends (win or draw). It alternates turns between the human player and the AI. The human player inputs their move through the console, and the AI uses the Q-learning algorithm to choose its actions. After each move, it checks for a win or a draw and updates the game state accordingly.\n",
    "\n",
    "Execution: The script starts by initializing the game state and running the main game loop. It continuously updates the board and switches between players until the game concludes with a win or a draw.\n",
    "\n",
    "This implementation of Tic-Tac-Toe using Q-learning allows the AI to learn from each game by updating the Q-values associated with the actions taken, thus gradually improving its gameplay strategy over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
